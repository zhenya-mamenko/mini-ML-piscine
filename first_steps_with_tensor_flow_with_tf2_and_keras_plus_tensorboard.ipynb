{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "first_steps_with_tensor_flow_with_tf2_and_keras_plus_tensorboard.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "JndnmDMp66FL",
        "ajVM7rkoYXeL",
        "ci1ISxxrZ7v0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhenya-mamenko/mini-ML-piscine/blob/master/first_steps_with_tensor_flow_with_tf2_and_keras_plus_tensorboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JndnmDMp66FL",
        "colab_type": "text"
      },
      "source": [
        "#### Copyright 2017 Google LLC., 2019 Zhenya Mamenko\n",
        "This notebook based on [First Steps with TensorFlow](https://colab.research.google.com/notebooks/mlcc/first_steps_with_tensor_flow.ipynb?utm_source=zhenya-mamenko&utm_campaign=colab-external&utm_medium=referral&utm_content=firststeps-colab&hl=en) exercise from [Google Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMqWDc_m6rUC",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f3CKqFUqL2-",
        "colab_type": "text"
      },
      "source": [
        "# First Steps with TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd2Zkk1LE2Zr",
        "colab_type": "text"
      },
      "source": [
        "**Learning Objectives:**\n",
        "  * Learn fundamental TensorFlow 2.0 and Keras concepts\n",
        "  * Use the `Sequential` model with `Dense` layer and `linear` activation function to predict median housing price, at the granularity of city blocks, based on one input feature\n",
        "  * Evaluate the accuracy of a model's predictions using Root Mean Squared Error (RMSE)\n",
        "  * Improve the accuracy of a model by tuning its hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxiIKhP4E2Zr",
        "colab_type": "text"
      },
      "source": [
        "The [data](https://developers.google.com/machine-learning/crash-course/california-housing-data-description) is based on 1990 census data from California."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TjLjL9IU80G",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "In this first cell, we'll load the `tensorflow 2.0.0-beta1` and other necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVFf5asKE2Zt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "from packaging import version\n",
        "from IPython.display import display\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.1f}'.format\n",
        "logging.getLogger('tensorflow').disabled = True\n",
        "\n",
        "!pip install -q tensorflow==2.0.0-beta1\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO_iSV1q9To0",
        "colab_type": "text"
      },
      "source": [
        "Make sure that our kernel use version 2.0 of TensorFlow. If not, reload kernel with `Ctrl+M .`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c21UOAj9TMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf.__version__)\n",
        "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
        "    \"This notebook requires TensorFlow 2.0 or above.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqNJ9tG28OmK",
        "colab_type": "text"
      },
      "source": [
        "We'll use TensorBoard to visualize our work. Load extention and useful libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_sTSLzh8PDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load TensorBoard extention\n",
        "%load_ext tensorboard\n",
        "\n",
        "from datetime import datetime\n",
        "import io\n",
        "logging.getLogger('tensorboard').disabled = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipRyUHjhU80Q",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll load our data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ivCDWnwE2Zx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "california_housing_dataframe = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\", sep=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVk_qlG6U80j",
        "colab_type": "text"
      },
      "source": [
        "We'll randomize the data, just to be sure not to get any pathological ordering effects that might harm the performance of Stochastic Gradient Descent. Additionally, we'll scale `median_house_value` to be in units of thousands, so it can be learned a little more easily with learning rates in a range that we usually use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0eVyguIU80m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "california_housing_dataframe = california_housing_dataframe.reindex(\n",
        "    np.random.permutation(california_housing_dataframe.index))\n",
        "california_housing_dataframe[\"median_house_value\"] /= 1000.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzzlSs3PtTmt",
        "colab_type": "text"
      },
      "source": [
        "## Examine the Data\n",
        "\n",
        "It's a good idea to get to know your data a little bit before you work with it.\n",
        "\n",
        "We'll print out a quick summary of a few useful statistics on each column: count of examples, mean, standard deviation, max, min, and various quantiles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzb10yoVrydW",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "california_housing_dataframe.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr6wYl2bt2Ep",
        "colab_type": "text"
      },
      "source": [
        "## Build the First Model\n",
        "\n",
        "In this exercise, we'll try to predict `median_house_value`, which will be our label (sometimes also called a target). We'll use `total_rooms` as our input feature.\n",
        "\n",
        "**NOTE:** Our data is at the city block level, so this feature represents the total number of rooms in that block.\n",
        "\n",
        "To train our model, we'll use the [Sequential](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Sequential) interface provided by the TensorFlow 2.0 [Keras](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras) API. This is high-level API for TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cpcsieFhsNI",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: Define Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL8-9d4ZJNR7",
        "colab_type": "text"
      },
      "source": [
        "There are two main types of data we'll use in this and future exercises:\n",
        "\n",
        "* **Categorical Data**: Data that is textual. In this exercise, our housing data set does not contain any categorical features, but examples you might see would be the home style, the words in a real-estate ad.\n",
        "\n",
        "* **Numerical Data**: Data that is a number (integer or float) and that you want to treat as a number. As we will discuss more later sometimes you might want to treat numerical data (e.g., a postal code) as if it were categorical.\n",
        "\n",
        "In Keras, we don't need any special preparations of our data, just get NumPy arrays from dataset.\n",
        "\n",
        "To start, we're going to use just one numeric input feature, `total_rooms`. The following code pulls the `total_rooms` data from our `california_housing_dataframe`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhEbFCZ86cDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the input feature: total_rooms.\n",
        "features = california_housing_dataframe[[\"total_rooms\"]].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_3S8teX7Rd2",
        "colab_type": "text"
      },
      "source": [
        "**NOTE:** The shape of our `total_rooms` data is a one-dimensional array (a list of the total number of rooms for each block)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMl3qrU5MGV6",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: Define the Label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw4nrfcB7kyk",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll define our label, which is `median_house_value`. Again, we can pull it from our `california_housing_dataframe`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1NvvNkH8Kbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the label.\n",
        "labels = california_housing_dataframe[\"median_house_value\"].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M-rTFHL2UkA",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: Configure the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eVVjmT2Drx1",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll configure a simple Keras model using `Sequential` class. We'll use only one [`Dense`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense) layer and than we'll train this model using the [Stohastic Gradient Descent](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers/SGD) optimizer `SGD`. The `learning_rate` argument controls the size of the gradient step.\n",
        "\n",
        "**NOTE:** To be safe, we also apply [gradient clipping](https://developers.google.com/machine-learning/glossary/#gradient_clipping) to our optimizer via `clipnorm` parameter. Gradient clipping ensures the magnitude of the gradients do not become too large during training, which can cause gradient descent to fail. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubhtW-NGU802",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use linear activation function to emulate LinearRegressor estimator from TF1\n",
        "model = tf.keras.models.Sequential([\n",
        "    layers.Dense(1, activation='linear', kernel_initializer='random_normal')\n",
        "])\n",
        "\n",
        "# Use gradient descent as the optimizer for compiile the model and MSE metric.\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0000001, clipnorm=5.0),\n",
        "              loss='mse',\n",
        "              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YS50CQb2ooO",
        "colab_type": "text"
      },
      "source": [
        "### Step 4: Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP92XkzhU803",
        "colab_type": "text"
      },
      "source": [
        "We can now call `fit()` on our `model` to train it, and to start, we'll train for 100 steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M-Kt6w8U803",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(features, labels, epochs=1, steps_per_epoch=100, batch_size=1, verbose=0).history\n",
        "root_mean_squared_error = history[\"root_mean_squared_error\"][0]\n",
        "mean_squared_error = history[\"loss\"][0]\n",
        "print(\"Mean Squared Error (on training data): %0.3f\" % mean_squared_error)\n",
        "print(\"Root Mean Squared Error (on training data): %0.3f\" % root_mean_squared_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoDaF2dlJQG5",
        "colab_type": "text"
      },
      "source": [
        "**NOTE:** Training error measures how well your model fits the training data, but it **_does not_** measure how well your model **_generalizes to new data_**. In later exercises, you'll explore how to split your data to evaluate your model's ability to generalize.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKWstXXPzOVz",
        "colab_type": "text"
      },
      "source": [
        "Is this a good model? How would you judge how large this error is?\n",
        "\n",
        "Mean Squared Error (MSE) can be hard to interpret, so we often look at Root Mean Squared Error (RMSE)\n",
        "instead.  A nice property of RMSE is that it can be interpreted on the same scale as the original targets.\n",
        "\n",
        "Let's compare the RMSE to the difference of the min and max of our targets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UwqGbbxP53O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_house_value = california_housing_dataframe[\"median_house_value\"].min()\n",
        "max_house_value = california_housing_dataframe[\"median_house_value\"].max()\n",
        "min_max_difference = max_house_value - min_house_value\n",
        "\n",
        "print(\"Min. Median House Value: %0.3f\" % min_house_value)\n",
        "print(\"Max. Median House Value: %0.3f\" % max_house_value)\n",
        "print(\"Difference between Min. and Max.: %0.3f\" % min_max_difference)\n",
        "print(\"Root Mean Squared Error: %0.3f\" % root_mean_squared_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JigJr0C7Pzit",
        "colab_type": "text"
      },
      "source": [
        "Our error is too big concerning to the target values. Can we do better?\n",
        "\n",
        "This is the question that nags at every model developer. Let's develop some basic strategies to reduce model error.\n",
        "\n",
        "The first thing we can do is take a look at how well our predictions match our targets, in terms of overall summary statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "941nclxbzqGH",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "calibration_data = pd.DataFrame()\n",
        "calibration_data[\"predictions\"] = model.predict_on_batch(california_housing_dataframe[\"total_rooms\"].values).flatten()\n",
        "calibration_data[\"targets\"] = pd.Series(labels)\n",
        "calibration_data.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2-bf8Hq36y8",
        "colab_type": "text"
      },
      "source": [
        "Okay, maybe this information is helpful. How does the mean value compare to the model's RMSE? How about the various quantiles?\n",
        "\n",
        "We can also visualize the data and the line we've learned.  Recall that linear regression on a single feature can be drawn as a line mapping input *x* to output *y*.\n",
        "\n",
        "First, we'll get a uniform random sample of the data so we can make a readable scatter plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGRIi3mAU81H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = california_housing_dataframe.sample(n=300)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-JwuJBKU81J",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll plot the line we've learned, drawing from the model's bias term and feature weight, together with the scatter plot. The line will show up red.\n",
        "We need helper function for saving our plots to TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-PfZBoz-Vl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_to_image(figure):\n",
        "  \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
        "  returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
        "  # Save the plot to a PNG in memory.\n",
        "  buf = io.BytesIO()\n",
        "  plt.savefig(buf, format='png')\n",
        "  # Closing the figure prevents it from being displayed directly inside\n",
        "  # the notebook.\n",
        "  plt.close(figure)\n",
        "  buf.seek(0)\n",
        "  # Convert PNG buffer to TF image\n",
        "  image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "  # Add the batch dimension\n",
        "  image = tf.expand_dims(image, 0)\n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gDEWTnI-qkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some magic: remove old plots\n",
        "\n",
        "!rm -rf logs/first_steps_with_tensor_flow_with_tf2_and_keras_plus_tensorboard/plots"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G12E76-339G",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Get the min and max total_rooms values.\n",
        "x_0 = sample[\"total_rooms\"].min()\n",
        "x_1 = sample[\"total_rooms\"].max()\n",
        "\n",
        "# Retrieve the final weight and bias generated during training.\n",
        "weight, bias = [x.flatten()[0] for x in model.layers[0].get_weights()]\n",
        "\n",
        "# Get the predicted median_house_values for the min and max total_rooms values.\n",
        "y_0 = weight * x_0 + bias \n",
        "y_1 = weight * x_1 + bias\n",
        "\n",
        "# Plot our regression line from (x_0, y_0) to (x_1, y_1).\n",
        "plt.plot([x_0, x_1], [y_0, y_1], c='r')\n",
        "\n",
        "# Label the graph axes.\n",
        "plt.ylabel(\"median_house_value\")\n",
        "plt.xlabel(\"total_rooms\")\n",
        "\n",
        "# Plot a scatter plot from our data sample.\n",
        "plt.scatter(sample[\"total_rooms\"], sample[\"median_house_value\"])\n",
        "\n",
        "logdir = \"logs/first_steps_with_tensor_flow_with_tf2_and_keras_plus_tensorboard/plots/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "file_writer = tf.summary.create_file_writer(logdir)\n",
        "figure = plt.figure(1)\n",
        "with file_writer.as_default():\n",
        "  tf.summary.image(\"Learned Line over scatter plot from sample data\",\n",
        "                   plot_to_image(figure),\n",
        "                   step=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwyYbqOMIfgf",
        "colab_type": "text"
      },
      "source": [
        "Run TensorBoard to show our graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcvwuoVcIbkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs/first_steps_with_tensor_flow_with_tf2_and_keras_plus_tensorboard/plots"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0lRt4USU81L",
        "colab_type": "text"
      },
      "source": [
        "This initial line looks way off.  See if you can look back at the summary stats and see the same information encoded there.\n",
        "\n",
        "Together, these initial sanity checks suggest we may be able to find a much better line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZWF67uv0HTG",
        "colab_type": "text"
      },
      "source": [
        "## Tweak the Model Hyperparameters\n",
        "For this exercise, we've write function for training model. You can call the function with different parameters to see the effect.\n",
        "\n",
        "In this function, we'll proceed in 10 epochs so that we can observe the model improvement at each epoch.\n",
        "\n",
        "For each epoch, we'll graph training loss.  This may help you judge when a model is converged, or if it needs more iterations.\n",
        "We'll also plot the feature weight and bias term values learned by the model over time.  This is another way to see how things converge.\n",
        "To do this, we'll implement simple [Lambda](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/LambdaCallback) [callback](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks).\n",
        "\n",
        "We'll also use [TensorBoard](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/TensorBoard) callback to log our RMSE and MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-Ii5jjaJ4x_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf logs/first_steps_with_tensor_flow_with_tf2_and_keras_plus_tensorboard/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO8J8-dY_UFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_model(learning_rate=0.001, steps_per_epoch=100, batch_size=1, input_feature=\"total_rooms\"):\n",
        "  \"\"\"Trains a linear regression model of one feature.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: A `float`, the learning rate.\n",
        "    steps_per_epoch: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    input_feature: A `string` specifying a column from `california_housing_dataframe`\n",
        "      to use as input feature.\n",
        "  \"\"\"\n",
        "  epochs = 10\n",
        "  features = california_housing_dataframe[[input_feature]].values\n",
        "  label = \"median_house_value\"\n",
        "  labels = california_housing_dataframe[label].values\n",
        "  \n",
        "  model = tf.keras.models.Sequential([\n",
        "      layers.Dense(1, activation='linear', kernel_initializer='random_normal', bias_initializer='random_normal')\n",
        "  ])\n",
        "  model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, clipnorm=5.0),\n",
        "                loss='mse',\n",
        "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "  \n",
        "  sample = california_housing_dataframe.sample(n=300)\n",
        "  logdir = \"logs/first_steps_with_tensor_flow_with_tf2_and_keras_plus_tensorboard/plots\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  scalars_logdir = \"logs/first_steps_with_tensor_flow_with_tf2_and_keras_plus_tensorboard/scalars\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  file_writer = tf.summary.create_file_writer(logdir)\n",
        "  \n",
        "  # Set up to plot the state of our model's line each epoch.\n",
        "  def create_plt_params(feature, label, epochs=10):\n",
        "    colors = [cm.coolwarm(x) for x in np.linspace(-1, 1, epochs)]\n",
        "    return (colors,\n",
        "            (sample[feature].min(), sample[feature].max()),\n",
        "            (0, sample[label].max()))\n",
        "    \n",
        "  def create_figure(feature, label, epochs=10):\n",
        "    figure = plt.figure(figsize=(15, 6))\n",
        "    plt.title(\"Learned Line by Epoch\")\n",
        "    plt.ylabel(label)\n",
        "    plt.xlabel(feature)\n",
        "    plt.scatter(sample[feature], sample[label])\n",
        "    return figure\n",
        "\n",
        "  colors, x_min_max, y_min_max = create_plt_params(input_feature, label, epochs)\n",
        "\n",
        "  def log(epoch, logs):\n",
        "    root_mean_squared_error = logs[\"root_mean_squared_error\"]\n",
        "    print(\"  epoch %02d : %0.2f\" % (epoch, root_mean_squared_error))\n",
        "\n",
        "    weight, bias = [x.flatten()[0] for x in model.layers[0].get_weights()]\n",
        "\n",
        "    # Apply some math to ensure that the data and line are plotted neatly.\n",
        "    y_extents = np.array(y_min_max)\n",
        "    x_extents = (y_extents - bias) / weight\n",
        "    x_extents = np.maximum(np.minimum(x_extents,\n",
        "                                      x_min_max[1]),\n",
        "                           x_min_max[0])\n",
        "    y_extents = weight * x_extents + bias\n",
        "    figure = create_figure(input_feature, label, epochs)\n",
        "    plt.plot(x_extents, y_extents, color=colors[epoch]) \n",
        "    with file_writer.as_default():\n",
        "      tf.summary.image(\"Learned Line by Epoch\",\n",
        "                       plot_to_image(figure),\n",
        "                       step=epoch)\n",
        "      \n",
        "  model_callback = tf.keras.callbacks.LambdaCallback(\n",
        "      on_epoch_end=lambda epoch, logs: log(epoch, logs))\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=scalars_logdir)\n",
        "  \n",
        "  print(\"Train model...\")\n",
        "  print(\"RMSE (on training data):\")\n",
        "  history = model.fit(features,\n",
        "            labels,\n",
        "            epochs=epochs,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=[model_callback, tensorboard_callback],\n",
        "            verbose=0).history\n",
        "  print(\"Model training finished.\")\n",
        "\n",
        "  calibration_data = pd.DataFrame()\n",
        "  calibration_data[\"predictions\"] = model.predict_on_batch(features).flatten()\n",
        "  calibration_data[\"targets\"] = pd.Series(labels)\n",
        "  display(calibration_data.describe())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg8A4ArBU81Q",
        "colab_type": "text"
      },
      "source": [
        "## Task 1:  Achieve an RMSE of 180 or Below\n",
        "\n",
        "Tweak the model hyperparameters to improve loss and better match the target distribution.\n",
        "If, after 5 minutes or so, you're having trouble beating a RMSE of 180, check the solution for a possible combination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzoZUSdLIolF",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "fit_model(learning_rate=0.000001,\n",
        "          steps_per_epoch=100,\n",
        "          batch_size=1,\n",
        "          input_feature=\"total_rooms\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTcAzEoQPTHi",
        "colab_type": "text"
      },
      "source": [
        "Run TensorBoard to show Learned Line and RMSE graphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evDDSiMoPSFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs/first_steps_with_tensor_flow_with_tf2_and_keras_plus_tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajVM7rkoYXeL",
        "colab_type": "text"
      },
      "source": [
        "### Solution\n",
        "\n",
        "Click below for one possible solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3zmldDwYy5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fit_model(learning_rate=0.00002,\n",
        "          steps_per_epoch=500,\n",
        "          batch_size=5,\n",
        "          input_feature=\"total_rooms\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8H0_D4vYa49",
        "colab_type": "text"
      },
      "source": [
        "This is just one possible configuration; there may be other combinations of settings that also give good results. Note that in general, this exercise isn't about finding the *one best* setting, but to help build your intutions about how tweaking the model configuration affects prediction quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU5sLyYTqzqL",
        "colab_type": "text"
      },
      "source": [
        "### Is There a Standard Heuristic for Model Tuning?\n",
        "\n",
        "This is a commonly asked question. The short answer is that the effects of different hyperparameters are data dependent. So there are no hard-and-fast rules; you'll need to test on your data.\n",
        "\n",
        "That said, here are a few rules of thumb that may help guide you:\n",
        "\n",
        " * Training error should steadily decrease, steeply at first, and should eventually plateau as training converges.\n",
        " * If the training has not converged, try running it for longer.\n",
        " * If the training error decreases too slowly, increasing the learning rate may help it decrease faster.\n",
        "   * But sometimes the exact opposite may happen if the learning rate is too high.\n",
        " * If the training error varies wildly, try decreasing the learning rate.\n",
        "   * Lower learning rate plus larger number of steps or larger batch size is often a good combination.\n",
        " * Very small batch sizes can also cause instability.  First try larger values like 100 or 1000, and decrease until you see degradation.\n",
        "\n",
        "Again, never go strictly by these rules of thumb, because the effects are data dependent.  Always experiment and verify."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpV-uF_cBCBU",
        "colab_type": "text"
      },
      "source": [
        "## Task 2: Try a Different Feature\n",
        "\n",
        "See if you can do any better by replacing the `total_rooms` feature with the `population` feature.\n",
        "\n",
        "Don't take more than 5 minutes on this portion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVav5d1HS2Ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf logs/first_steps_with_tensor_flow_with_tf2_and_keras_plus_tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMyOxzb0ZlAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fit_model(learning_rate=0.00001,\n",
        "          steps_per_epoch=500,\n",
        "          batch_size=10,\n",
        "          input_feature=\"population\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z44RhUiSqWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs/first_steps_with_tensor_flow_with_tf2_and_keras_plus_tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci1ISxxrZ7v0",
        "colab_type": "text"
      },
      "source": [
        "### Solution\n",
        "\n",
        "Click below for one possible solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjdQQCduZ7BV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fit_model(learning_rate=0.00002,\n",
        "          steps_per_epoch=1000,\n",
        "          batch_size=5,\n",
        "          input_feature=\"population\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}